{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"XG2fIyUok-Xs"},"outputs":[],"source":["import os\n","import csv\n","import glob\n","import json\n","import numpy as np\n","np.set_printoptions(threshold = np.inf)\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","from tensorflow.keras import layers, models, losses, optimizers, metrics, callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E4xaUKpEk-Xu"},"outputs":[],"source":["MAX_SPLIT_SIZE = 24\n","SAVE_DIR = \"results/\"\n","if not os.path.exists(SAVE_DIR):\n","    os.mkdir(SAVE_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IASeNNzFk-Xv"},"outputs":[],"source":["def read_csv(csv_path):\n","    datas = list()\n","\n","    with open(csv_path, \"r\") as f:\n","        r = csv.reader(f)\n","\n","        for i, R in enumerate(r):\n","            if i == 0: continue\n","\n","            if i == 1:\n","                next_peak = int(float(R[2]))\n","                continue\n","\n","            peak = int(float(R[2]))\n","            gain = next_peak - peak\n","            gain_percent = (next_peak - peak) / peak * 100\n","\n","            tmp = {\n","                \"year\" : int(R[0]),\n","                \"month\" : int(R[1]),\n","                \"peak\" : peak,\n","                \"price\" : int(R[4]),\n","                \"steam_online\" : int(R[5]),\n","                \"gain\" : gain,\n","                \"gain_percent\" : gain_percent\n","            }\n","            datas.append(tmp)\n","\n","            next_peak = peak\n","\n","    return list(reversed(datas))\n","\n","def standardize_data(datas):\n","    items = list(datas[-1].keys())\n","\n","    mean_items = {\n","        I : np.average([D[I] for D in datas]) for I in items\n","    }\n","\n","    std_items = {\n","        I : np.std([D[I] for D in datas]) for I in items\n","    }\n","\n","    sd_datas = list()\n","    for D in datas:\n","        tmp = dict()\n","        for I in items:\n","            if std_items[I] == 0:\n","                tmp[I] = 0\n","            else:\n","                tmp[I] = (D[I] - mean_items[I]) / std_items[I]\n","        sd_datas.append(tmp)\n","\n","    return sd_datas, mean_items, std_items"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kzMFaC7kk-Xw"},"outputs":[],"source":["class MyMetric(metrics.Metric):\n","    def __init__(self):\n","        super().__init__(name = \"MyMetric\")\n","        self.sum = self.add_weight(name = \"sum\", initializer = \"zeros\")\n","        self.count = self.add_weight(name = \"count\", initializer = \"zeros\")\n","\n","    def update_state(self, y_true, y_pred, sample_weight = None):\n","        tf.print(y_true.shape, y_pred.shape)\n","        self.count.assign_add(tf.cast(tf.shape(y_true)[0], tf.float32))\n","        tmp = tf.math.reduce_sum(tf.math.pow(y_true[:, -1] - y_pred[:, -1], 2))\n","        self.sum.assign_add(tmp)\n","\n","    def result(self):\n","        tf.print(self.count)\n","        return self.sum / 10 # self.count\n","\n","    def reset_state(self):\n","        self.sum.assign(0)\n","        self.count.assign(0)\n","\n","    def reset_states(self):\n","        self.sum.assign(0)\n","        self.count.assign(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YipjhoaMk-Xx"},"outputs":[],"source":["csgo_datas = read_csv(\"csgo_dataset.csv\")\n","sd_csgo_datas, mean_csgo, std_csgo = standardize_data(csgo_datas)\n","\n","dota2_datas = read_csv(\"dota2_dataset.csv\")\n","sd_dota2_datas, mean_dota2, std_dota2 = standardize_data(dota2_datas)\n","\n","rust_datas = read_csv(\"Rust_dataset.csv\")\n","sd_rust_datas, mean_rust, std_rust = standardize_data(rust_datas)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y05GQSJ7k-Xx"},"outputs":[],"source":["inputs = list()\n","teacher_signals = list()\n","\n","for i in range(MAX_SPLIT_SIZE, len(sd_csgo_datas)):\n","    tmp_inputs = list()\n","    tmp_teacher_signals = list()\n","\n","    for D in sd_csgo_datas[i - MAX_SPLIT_SIZE : i]:\n","        tmp = list()\n","        tmp.append( D[\"peak\"] )\n","        tmp.append( D[\"price\"] )\n","        tmp.append( D[\"steam_online\"] )\n","\n","        tmp_inputs.append(tmp)\n","        tmp_teacher_signals.append(D[\"gain_percent\"])\n","\n","    inputs.append( tmp_inputs )\n","    teacher_signals.append( tmp_teacher_signals )\n","\n","inputs = np.array(inputs)\n","teacher_signals = np.array(teacher_signals)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PsK5CPl8k-Xy","outputId":"86b61557-4a7c-40d5-8b69-ddf4e873396a"},"outputs":[{"data":{"text/plain":["((85, 24, 3), (85, 24), (10, 24, 3), (10, 24))"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["train_index, test_index = train_test_split(list(range(len(inputs))), test_size = 0.1)\n","train_inputs, train_teacher_signals = inputs[train_index], teacher_signals[train_index]\n","test_inputs, test_teacher_signals = inputs[test_index], teacher_signals[test_index]\n","\n","#\n","train_inputs.shape, train_teacher_signals.shape, test_inputs.shape, test_teacher_signals.shape"]},{"cell_type":"markdown","metadata":{"id":"ndQArkyRk-Xz"},"source":["return_sequence : True\n","\n","batch_size : フルバッチ\n","\n","中間層 : 1層"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"woUB9vNyk-X1"},"outputs":[],"source":["return_sequence = True\n","batch_size = len(train_index)\n","num_middle = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYS2PVCBk-X1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5XxYIvek-X1","outputId":"dacec1b4-fe15-4cd0-d29d-7608a7a2743a"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/24 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["TensorShape([85, 19]) TensorShape([85, 19, 1])\n","85\n","1/1 [==============================] - ETA: 0s - loss: 0.8093 - MyMetric: 448.5855TensorShape([None, 19]) TensorShape([None, 19, 1])\n","10\n","1/1 [==============================] - 1s 984ms/step - loss: 0.8093 - MyMetric: 448.5855 - val_loss: 0.4575 - val_MyMetric: 4.9282\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██        | 5/24 [00:01<00:05,  3.78it/s]\n"]}],"source":["for i in tqdm(range(MAX_SPLIT_SIZE, 0, -1)):\n","\n","    target_dir = f\"{SAVE_DIR}RNN{num_middle}_{return_sequence}_{batch_size}_{i}/\"\n","    if os.path.exists(target_dir):\n","        continue\n","    os.mkdir(target_dir)\n","\n","    tmp_train_inputs = train_inputs[:, -i:, :]\n","    tmp_train_teacher_signals = train_teacher_signals[:, -i:]\n","    tmp_test_inputs = test_inputs[:, -i:, :]\n","    tmp_test_teacher_signals = test_teacher_signals[:, -i:]\n","\n","    def build_model():\n","        input = layers.Input(shape = tmp_train_inputs.shape[1:])\n","        x = layers.SimpleRNN(units = 1024, activation = \"relu\", return_sequences = return_sequence)(input)\n","        output = layers.Dense(units = 1, activation = \"linear\")(x)\n","\n","        return models.Model(input, output)\n","\n","    mcp = callbacks.ModelCheckpoint(filepath = f\"{target_dir}/model.keras\",\n","                                    monitor = \"val_loss\",\n","                                    mode = \"min\",\n","                                    save_best_only = True)\n","    model = build_model()\n","    model.compile(loss = losses.MeanSquaredError(),\n","                  optimizer = optimizers.Adam(learning_rate = 0.0001),\n","                  metrics = MyMetric())\n","    history = model.fit(x = tmp_train_inputs, y = tmp_train_teacher_signals, batch_size = batch_size, epochs = 1,\n","                        validation_data = (tmp_test_inputs, tmp_test_teacher_signals), callbacks = [mcp])\n","\n","    with open(f\"{target_dir}/history.json\", \"w\") as f:\n","        json.dump(history.history, f)\n","\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cWSwhvk8k-X2","outputId":"222883eb-1048-4de1-dbf5-2da15378d692"},"outputs":[{"data":{"text/plain":["{'loss': [0.8092679977416992],\n"," 'MyMetric': [448.5855407714844],\n"," 'val_loss': [0.45748835802078247],\n"," 'val_MyMetric': [4.928160190582275]}"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["history.history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fLJ5hIQPk-X2"},"outputs":[],"source":["p = model(tmp_test_inputs)[:, :, 0]\n","a, b = tmp_test_teacher_signals[:, -1], p[:, -1]\n","A, B = tmp_test_teacher_signals, p"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vtCP51zfk-X2","outputId":"5db1415f-6743-4a8e-9cc1-b885a2bc7a73"},"outputs":[{"data":{"text/plain":["TensorShape([10, 19])"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["p.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5utNZuDjk-X2","outputId":"59b69bf1-6d63-4617-cf1b-8e25858f1046"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 19], dtype=int32)>"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["tf.shape(p)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iKOs7eik-X3","outputId":"15129984-383a-4ae8-d10e-94361f225bd4"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=0.44242358>"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["losses.MeanSquaredError()(a, b)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mI_DLDvyk-X3","outputId":"cfd2a4cc-9b3f-4811-8bb1-557b2ed748d6"},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=0.44242358>"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["tf.math.reduce_sum(tf.math.pow(a - b, 2)) / len(a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTqd9T5xk-X3","outputId":"ea4fd73b-4c3d-4a6f-d7de-f4a205dbe1e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 19) TensorShape([10, 19])\n","10\n"]},{"data":{"text/plain":["<tf.Tensor: shape=(), dtype=float32, numpy=0.44242358>"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["c = MyMetric()\n","c.update_state(A, B)\n","c.result()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ld_452ddk-X4","outputId":"2aff4676-ce55-4b02-db52-e114eb55cea8"},"outputs":[{"name":"stdout","output_type":"stream","text":["TensorShape([None, 19]) TensorShape([None, 19, 1])\n","10\n","WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7fcf3ae82430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"data":{"text/plain":["[0.45748835802078247, 4.928160190582275]"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["model.train_on_batch(tmp_test_inputs, tmp_test_teacher_signals)"]}],"metadata":{"kernelspec":{"display_name":"tf291","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}